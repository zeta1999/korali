#include "modules/conduit/conduit.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/solver/deepLearner/deepLearner.hpp"

namespace korali
{
namespace solver
{
void DeepLearner::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::SupervisedLearning *>(_k->_problem);

  if (_problem->_trainingBatchSize == 0) KORALI_LOG_ERROR("Training data has not been provided for variable 0.\n");
  if (_problem->_validationBatchSize == 0) KORALI_LOG_ERROR("Validation data has not been provided for variable 0.\n");

  // Setting training NN
  _trainingNeuralNetwork = dynamic_cast<NeuralNetwork *>(Module::duplicate(_neuralNetwork));

  // Setting input training data
  _trainingNeuralNetwork->_layers[0]->_nodeValues.resize(_problem->_trainingBatchSize);
  for (size_t i = 0; i < _problem->_trainingBatchSize; i++)
  {
    _trainingNeuralNetwork->_layers[0]->_nodeValues[i].resize(_problem->_inputVectorSize);
    for (size_t j = 0; j < _problem->_inputVectorSize; j++)
    {
      size_t varIdx = _problem->_inputVectorIndexes[j];
      _trainingNeuralNetwork->_layers[0]->_nodeValues[i][j] = _k->_variables[varIdx]->_trainingData[i];
    }
  }

  // Setting Solution for NN training and validation
  _trainingNeuralNetwork->_solution.resize(_problem->_trainingBatchSize);
  for (size_t i = 0; i < _problem->_trainingBatchSize; i++)
  {
    _trainingNeuralNetwork->_solution[i].resize(_problem->_outputVectorSize);
    for (size_t j = 0; j < _problem->_outputVectorSize; j++)
    {
      size_t varIdx = _problem->_outputVectorIndexes[j];
      _trainingNeuralNetwork->_solution[i][j] = _k->_variables[varIdx]->_trainingData[i];
    }
  }

  // Creating Training Neural Network internal structures
  _trainingNeuralNetwork->_operation = "Training";
  _trainingNeuralNetwork->create();

  // Creating Validation NN
  _validationNeuralNetwork = dynamic_cast<NeuralNetwork *>(Module::duplicate(_trainingNeuralNetwork));
  _validationNeuralNetwork->_operation = "Inference";

  // Setting input validation data
  _validationNeuralNetwork->_layers[0]->_nodeValues.resize(_problem->_validationBatchSize);
  for (size_t i = 0; i < _problem->_validationBatchSize; i++)
  {
    _validationNeuralNetwork->_layers[0]->_nodeValues[i].resize(_problem->_inputVectorSize);
    for (size_t j = 0; j < _problem->_inputVectorSize; j++)
    {
      size_t varIdx = _problem->_inputVectorIndexes[j];
      _validationNeuralNetwork->_layers[0]->_nodeValues[i][j] = _k->_variables[varIdx]->_validationData[i];
    }
  }

  _validationNeuralNetwork->_solution.resize(_problem->_validationBatchSize);
  for (size_t i = 0; i < _problem->_validationBatchSize; i++)
  {
    _validationNeuralNetwork->_solution[i].resize(_problem->_outputVectorSize);
    for (size_t j = 0; j < _problem->_outputVectorSize; j++)
    {
      size_t varIdx = _problem->_outputVectorIndexes[j];
      _validationNeuralNetwork->_solution[i][j] = _k->_variables[varIdx]->_validationData[i];
    }
  }

  // Creating Validation Neural Network internal structures, keeping normalization from training NN
  _validationNeuralNetwork->create();
  _validationNeuralNetwork->_operation = "Inference";

  // Creating Test NN
  _testNeuralNetwork = dynamic_cast<NeuralNetwork *>(Module::duplicate(_neuralNetwork));

}

void DeepLearner::runGeneration()
{
  /**************************************************************
  * Training Stage
  *************************************************************/

  _trainingNeuralNetwork->train();

  /**************************************************************
  * Validation Stage
  *************************************************************/

  // Setting input validation data
  Sample validationSample;
  validationSample["Parameters"] = _trainingNeuralNetwork->_validationParameters;
  NeuralNetwork::runSample(validationSample, _validationNeuralNetwork);

  // Getting results of optimization
  _currentValidationLoss = -KORALI_GET(double, validationSample, "F(x)");

  // If validation is better, saving it as the best network for use as for test batch later.
  _currentInactiveSteps++;
  if (_currentValidationLoss < _lowestValidationLoss)
  {
    // Reseting inactive counter
    _currentInactiveSteps = 0;

    // Saving lowest validation loss
    _lowestValidationLoss = _currentValidationLoss;

    // Storing best NN
    knlohmann::json js;
    _validationNeuralNetwork->getConfiguration(js);
    _testNeuralNetwork->_layers.clear();
    _testNeuralNetwork->setConfiguration(js);
    _testNeuralNetwork->_solution.clear();
  }
}

std::vector<std::vector<double>> DeepLearner::evaluate(const std::vector<std::vector<double>> &inputBatch)
{
  // Re-creating NN's internal structures
  _testNeuralNetwork->_layers[0]->_nodeValues = inputBatch;
  _testNeuralNetwork->create();

  // Updating the network's weights and biases
  _testNeuralNetwork->updateWeightsAndBias();

  // Running the input values through the neural network
  _testNeuralNetwork->forward();

  // Denormalizing values
  size_t outputLayerId = _testNeuralNetwork->_layers.size() - 1;
  size_t batchSize = _testNeuralNetwork->_layers[outputLayerId]->_nodeValues.size();
  size_t outputSize = _testNeuralNetwork->_layers[outputLayerId]->_nodeValues[0].size();

  // If training was normalized, we need to de-normalize the results
  if (_testNeuralNetwork->_batchNormalizationEnabled == true)
  {
    for (size_t i = 0; i < batchSize; i++)
      for (size_t j = 0; j < outputSize; j++)
      {
        //double value = _testNeuralNetwork->_layers[outputLayerId]->_nodeValues[i][j];
        double sigma = _testNeuralNetwork->_outputNormalizationSigmas[j];
        double mean = _testNeuralNetwork->_outputNormalizationMeans[j];
        _testNeuralNetwork->_layers[outputLayerId]->_nodeValues[i][j] *= sigma;
        _testNeuralNetwork->_layers[outputLayerId]->_nodeValues[i][j] += mean;
      }
  }

  return _testNeuralNetwork->_layers[outputLayerId]->_nodeValues;
}

void DeepLearner::printGenerationAfter()
{
  // Printing results so far
  _k->_logger->logInfo("Normal", "Training Loss: %.15f\n", _trainingNeuralNetwork->_currentTrainingLoss);
  _k->_logger->logInfo("Normal", "Current Validation Loss: %.15f\n", _currentValidationLoss);
  _k->_logger->logInfo("Normal", "Lowest Validation Loss: %.15f\n", _lowestValidationLoss);
  _k->_logger->logInfo("Normal", "Inactive Step Counter: %lu\n", _currentInactiveSteps);
}

} // namespace solver

} // namespace korali
