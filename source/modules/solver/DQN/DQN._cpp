#include "modules/conduit/conduit.hpp"
#include "modules/solver/DQN/DQN.hpp"

void korali::solver::DQN::runGeneration()
{
  // If initial generation, get problem pointer, and set initial DQN configuration
  if (_k->_currentGeneration == 1)
  {
    _problem = dynamic_cast<korali::problem::ReinforcementLearning *>(_k->_problem);
    setInitialConfiguration();
  }

  // Instructing DQN to initialize generation configuration
  initializeGeneration();

  // Creating storage for agents
  std::vector<korali::Sample> agents(_agentCount);

  // Index for current worker Id that helps making a balanced distribution of work
  size_t currentWorker = 0;

  // Initializing the agents and their environments
  for (size_t i = 0; i < _agentCount; i++)
  {
    // Configuring Agent
    agents[i]["Worker"] = 0;
    agents[i]["Sample Id"] = i;
    agents[i]["Module"] = "Problem";
    agents[i]["Operation"] = "Run Environment";

    // Launching agent initialization
    _conduit->start(agents[i]);

    // Advancing to the next worker (if reached the last, start from beginning)
    currentWorker++;
    if (currentWorker == _conduit->maxConcurrency()) currentWorker = 0;
  }

  // Waiting for all agents to initialize
  _conduit->waitAll(agents);

  // Storage for agent information
  std::vector<std::vector<double>> actions(_agentCount);
  std::vector<std::vector<double>> states(_agentCount);
  std::vector<double> rewards(_agentCount);
  std::vector<bool> hasFinished(_agentCount);

  // Storage for the entire history of the current policy
  std::vector<std::vector<std::vector<double>>> currentPolicyActions(_agentCount);
  std::vector<std::vector<std::vector<double>>> currentPolicyStates(_agentCount);

  // Storing agent's initial state and performing first action
  for (size_t i = 0; i < _agentCount; i++)
    if (hasFinished[i] == false)
    {
      try
      {
        states[i] = agents[i]["State"].get<std::vector<double>>();
      }
      catch (const std::exception &e)
      {
        korali::Logger::logError("DQN: Missing or incorrect value of 'State' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its result, e.g., sample[\"State\"] = value.\n   + Cause: %s\n", e.what());
      }

      // Getting new action to perform
      actions[i] = getAction(states[i]);
      agents[i]["Action"] = actions[i];

      // Running a new episode
      _conduit->start(agents[i]);
    }

  // Initializing Finished count
  size_t finishedCount = 0;

  // Running main iteration loop until all agents have finished
  while (finishedCount < _agentCount)
  {
    // Waiting for any of the pending agents
    size_t i = _conduit->waitAny(agents);

    // Add state/action to current policy history
    currentPolicyActions[i].push_back(actions[i]);
    currentPolicyStates[i].push_back(states[i]);

    // Updating reward
    try
    {
      rewards[i] = agents[i]["Reward"];
    }
    catch (const std::exception &e)
    {
      korali::Logger::logError("DQN: Missing or incorrect value of 'Reward' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its result, e.g., sample[\"State\"] = value.\n   + Cause: %s\n", e.what());
    }

    // Storing agent's new state
    std::vector<double> newState;
    try
    {
      newState = agents[i]["State"].get<std::vector<double>>();
    }
    catch (const std::exception &e)
    {
      korali::Logger::logError("DQN: Missing or incorrect value of 'State' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its result, e.g., sample[\"State\"] = value.\n   + Cause: %s\n", e.what());
    }

    // Storing the agent's experience into the learner
    storeExperience(states[i], actions[i], rewards[i], newState);

    // The new state becomes the current state
    states[i] = newState;

    hasFinished[i] = agents[i]["Finished"];

    // If it finished, then add to the count
    if (hasFinished[i])
    {
      // Adding the end state to the policy
      currentPolicyStates[i].push_back(states[i]);

      // Incrementing finished agent count
      finishedCount++;
    }
    else // Else, get new action and run next episode
    {
      // Getting new action to perform
      actions[i] = getAction(states[i]);
      agents[i]["Action"] = actions[i];

      // Run Episode
      _conduit->start(agents[i]);
    }
  }

  // Find and store best policy so far
  for (size_t i = 0; i < _agentCount; i++)
    if (rewards[i] > _bestReward)
    {
      // Saving best reward so far
      _bestReward = rewards[i];

      _bestPolicyStates = currentPolicyStates[i];
      _bestPolicyActions = currentPolicyActions[i];

      _k->_js["Results"]["Optimal Policy Actions"] = _bestPolicyActions;
      _k->_js["Results"]["Optimal Policy States"] = _bestPolicyStates;
      _k->_js["Results"]["Optimal Reward"] = _bestReward;
    }

  // Instructing DQN to process finsihed generation
  processGeneration();
}
