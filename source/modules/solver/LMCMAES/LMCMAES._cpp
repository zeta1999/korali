#include "modules/solver/LMCMAES/LMCMAES.hpp"
#include "sample/sample.hpp"
#include "modules/conduit/conduit.hpp"

#include <stdio.h>
#include <unistd.h>
#include <chrono>
#include <numeric>   // std::iota
#include <algorithm> // std::sort

void korali::solver::LMCMAES::setInitialConfiguration()
{
 if (_targetDistanceCoefficients.size() == 0) _targetDistanceCoefficients = { double(_k->_variables.size()), 0.0, 0.0 };

 if(_targetDistanceCoefficients.size() != 3)
  _k->_logger->logError("LMCMAES requires 3 parameters for 'Target Distance Coefficients' (%zu provided).\n", _targetDistanceCoefficients.size());

 if (_muValue == 0)  _muValue = _populationSize / 2;
 if (_cumulativeCovariance == 0.0) _cumulativeCovariance = 0.5/std::sqrt((double)_k->_variables.size());
 if (_choleskyMatrixLearningRate == 0.0) _choleskyMatrixLearningRate = 1.0/(10.0-std::log((double)_k->_variables.size()+1.0));
 if (_targetDistanceCoefficients.empty()) _targetDistanceCoefficients = { (double)_k->_variables.size(), 0.0, 0.0 };
 if (_setUpdateInterval == 0) _setUpdateInterval = std::max(std::floor(std::log(_k->_variables.size())), 1.0);
 if (_subsetSize == 0) _subsetSize = std::ceil(std::sqrt(double(_k->_variables.size())));

 _chiSquareNumber = sqrt((double) _k->_variables.size()) * (1. - 1./(4.*_k->_variables.size()) + 1./(21.*_k->_variables.size()*_k->_variables.size()));
 _sigmaExponentFactor = 0.0;
 _conjugateEvolutionPathL2Norm = 0.0;

 // Allocating Memory
 _samplePopulation.resize(_populationSize);
 for(size_t i = 0; i < _populationSize; i++) _samplePopulation[i].resize(_k->_variables.size());

 _evolutionPath.resize(_k->_variables.size());
 _meanUpdate.resize(_k->_variables.size());
 _currentMean.resize(_k->_variables.size());
 _previousMean.resize(_k->_variables.size());
 _bestEverVariables.resize(_k->_variables.size());
 _currentBestVariables.resize(_k->_variables.size());
 _randomVector.resize(_k->_variables.size());
 _choleskyFactorVectorProduct.resize(_k->_variables.size());
 _standardDeviation.resize(_k->_variables.size());
 
 _muWeights.resize(_muValue);
 
 _sortingIndex.resize(_populationSize);
 _previousSortingIndex.resize(_populationSize);
 _valueVector.resize(_populationSize);
 _previousValueVector.resize(_populationSize);

 _evolutionPathWeights.resize(_subsetSize);
 _subsetHistory.resize(_subsetSize);
 _subsetUpdateTimes.resize(_subsetSize);
   
 std::fill(_evolutionPathWeights.begin(), _evolutionPathWeights.end(), 0.0);
 std::fill(_subsetHistory.begin(), _subsetHistory.end(), 0);
 std::fill(_subsetUpdateTimes.begin(), _subsetUpdateTimes.end(), 0);
 
 _inverseVectors.resize(_subsetSize);
 _evolutionPathHistory.resize(_subsetSize);
 for(size_t i = 0 ; i < _subsetSize; ++i)
 {
   _inverseVectors[i].resize(_k->_variables.size());
   _evolutionPathHistory[i].resize(_k->_variables.size());
   std::fill(_inverseVectors[i].begin(), _inverseVectors[i].end(), 0.0);
   std::fill(_evolutionPathHistory[i].begin(), _evolutionPathHistory[i].end(), 0.0);
 }

 // Initializing variable defaults
 for (size_t i = 0; i < _k->_variables.size(); i++)
 {
   /* confirm lower & upper bound */
//   if( (std::isfinite(_k->_variables[i]->_lowerBound) && std::isfinite(_k->_variables[i]->_upperBound) ) == false )
//        _k->_logger->logError("Lower and/or upper bound of variable \'%s\' is not defined.\n", _k->_variables[i]->_name.c_str());

   /* init mean if not defined */
   if(std::isfinite(_k->_variables[i]->_initialMean) == false)
   {
    if(std::isfinite(_k->_variables[i]->_lowerBound) == false) _k->_logger->logError("Initial Mean of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
    if(std::isfinite(_k->_variables[i]->_upperBound) == false) _k->_logger->logError("Initial Mean of variable \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
    _k->_variables[i]->_initialMean = (_k->_variables[i]->_upperBound + _k->_variables[i]->_lowerBound)*0.5;
   }

   /* calculate stddevs */
   if( std::isfinite(_k->_variables[i]->_initialStandardDeviation) == false)
    _standardDeviation[i] = 0.3 * (_k->_variables[i]->_upperBound - _k->_variables[i]->_lowerBound);
   else
    _standardDeviation[i] = _k->_variables[i]->_initialStandardDeviation;
 }
 
 _sigma = _initialSigma;
 
 if ( (_sigmaUpdateRule == "CMAES" || _sigmaUpdateRule == "LMCMA") == false)
   _k->_logger->logError("Invalid setting of Sigma Update Rule (%s) (choose CMAES, or LMCMA).",  _sigmaUpdateRule.c_str());

 if      (_muType == "Linear")       for (size_t i = 0; i < _muValue; i++) _muWeights[i] = _muValue - i;
 else if (_muType == "Equal")        for (size_t i = 0; i < _muValue; i++) _muWeights[i] = 1.;
 else if (_muType == "Logarithmic")  for (size_t i = 0; i < _muValue; i++) _muWeights[i] = log(std::max( (double)_muValue, 0.5*_populationSize)+0.5)-log(i+1.);
 else  _k->_logger->logError("Invalid setting of Mu Type (%s) (Linear, Equal, and Logarithmic accepted).",  _muType.c_str());

 if ((_randomNumberDistribution != "Normal") && (_randomNumberDistribution != "Uniform")) _k->_logger->logError("Invalid setting of Random Number Distribution (%s) (Normal and Uniform accepted).",  _randomNumberDistribution.c_str());

 // Normalize weights vector and set mueff
 double s1 = 0.0;
 double s2 = 0.0;

 for (size_t i = 0; i < _muValue; i++)
 {
  s1 += _muWeights[i];
  s2 += _muWeights[i]*_muWeights[i];
 }
 _effectiveMu = s1*s1/s2;

 for (size_t i = 0; i < _muValue; i++) _muWeights[i] /= s1;

 if (_initialSigma <= 0.0)
     _k->_logger->logError("Invalid Initial Sigma (must be greater 0.0, is %lf).", _initialSigma);
 if (_cumulativeCovariance <= 0.0)
     _k->_logger->logError("Invalid Initial Cumulative Covariance (must be greater 0.0).");
 if (_sigmaCumulationFactor <= 0.0)
     _k->_logger->logError("Invalid Sigma Cumulative Covariance (must be greater 0.0).");
 if (_dampFactor <= 0.0)
     _k->_logger->logError("Invalid Damp Factor (must be greater 0.0).");
 if (_choleskyMatrixLearningRate <= 0.0 || _choleskyMatrixLearningRate > 1.0)
     _k->_logger->logError("Invalid Cholesky Matrix Learning Rate (must be in (0, 1], is %lf).", _choleskyMatrixLearningRate);
 if (_setUpdateInterval <= 0.0 )
     _k->_logger->logError("Invalid Set Update Interval(must be greater 0, is %zu).", _setUpdateInterval);


 _infeasibleSampleCount   = 0;
 _bestEverValue           = -std::numeric_limits<double>::infinity();
 _sqrtInverseCholeskyRate = std::sqrt(1.0-_choleskyMatrixLearningRate);

 for (size_t i = 0; i < _k->_variables.size(); i++) _currentMean[i] = _previousMean[i] = _k->_variables[i]->_initialMean;

}

void korali::solver::LMCMAES::runGeneration()
{
 if (_k->_currentGeneration == 1) setInitialConfiguration();

 prepareGeneration();

 // Initializing Sample Evaluation
 std::vector<korali::Sample> samples(_populationSize);
 for (size_t i = 0; i < _populationSize; i++)
 {
  samples[i]["Module"] = "Problem";
  samples[i]["Operation"]  = "Evaluate";
  samples[i]["Parameters"] = _samplePopulation[i];
  samples[i]["Sample Id"]  = i;
  _modelEvaluationCount++;
  _conduit->start(samples[i]);
 }

 // Waiting for samples to finish
 _conduit->waitAll(samples);
 updateDistribution(samples);
}


void korali::solver::LMCMAES::prepareGeneration()
{
 for (size_t i = 0; i < _populationSize; ++i)
 {
   bool isSampleFeasible;
   do
   {
    _infeasibleSampleCount++;
    sampleSingle(i);
    korali::Sample sample;
    sample["Module"] = "Problem";
    sample["Parameters"] = _samplePopulation[i];
    sample["Operation"] = "Check Feasibility";
    _conduit->start(sample);
    _conduit->wait(sample);
    isSampleFeasible = sample["Is Feasible"];
   }
   while(isSampleFeasible == false);

   _infeasibleSampleCount--;
 }
}

void korali::solver::LMCMAES::sampleSingle(size_t sampleIdx)
{
  if ( _symmetricSampling || (sampleIdx % 2) == 0)
  {
    choleskyFactorUpdate(sampleIdx);
    for (size_t d = 0; d < _k->_variables.size(); ++d) {
     _samplePopulation[sampleIdx][d] = _currentMean[d] + _sigma*_standardDeviation[d]*_choleskyFactorVectorProduct[d];
    }
  }
  else
  {
    for (size_t d = 0; d < _k->_variables.size(); ++d)
     //_samplePopulation[sampleIdx][d] = 2.0*_currentMean[d] - _samplePopulation[sampleIdx-1][d]; // version from [Loshchilov2015]
     _samplePopulation[sampleIdx][d] = _currentMean[d] - _sigma*_standardDeviation[d]*_choleskyFactorVectorProduct[d];  // version from Loshcholov's code
  }

}

void korali::solver::LMCMAES::updateDistribution(std::vector<korali::Sample> & samples)
{
 // Processing results
 for (size_t i = 0; i < _populationSize; i++)
  _valueVector[i] = samples[i]["F(x)"];

 /* Copy to previous */
 std::copy(_sortingIndex.begin(), _sortingIndex.end(), _previousSortingIndex.begin());
 std::copy(_valueVector.begin(), _valueVector.end(), _previousValueVector.begin());
 
 /* Generate _sortingIndex */
 sort_index(_valueVector, _sortingIndex);

 /* update function value history */
 _previousBestValue = _currentBestValue;

 /* update current best */
 _currentBestValue = _valueVector[0];
 for (size_t d = 0; d < _k->_variables.size(); ++d) _currentBestVariables[d] = _samplePopulation[_sortingIndex[0]][d];

 /* update xbestever */
 if ( _currentBestValue > _bestEverValue )
 {
  _previousBestEverValue = _bestEverValue;
  _bestEverValue         = _currentBestValue;
  (*_k)["Results"]["Best Sample"] = samples[_sortingIndex[0]]._js.getJson();

  for (size_t d = 0; d < _k->_variables.size(); ++d) _bestEverVariables[d] = _currentBestVariables[d];
 }

 /* set weights */
 for (size_t d = 0; d < _k->_variables.size(); ++d) {
   _previousMean[d] = _currentMean[d];
   _currentMean[d]  = 0.;
   for (size_t i = 0; i < _muValue; ++i)
     _currentMean[d] += _muWeights[i] * _samplePopulation[_sortingIndex[i]][d];

   _meanUpdate[d] = (_currentMean[d] - _previousMean[d])/(_sigma*_standardDeviation[d]);
 }

 /* update evolution path */
 _conjugateEvolutionPathL2Norm = 0.0;
 for (size_t d = 0; d < _k->_variables.size(); ++d)
 {
   _evolutionPath[d] = (1. - _cumulativeCovariance) * _evolutionPath[d] + sqrt( _cumulativeCovariance * (2. - _cumulativeCovariance) * _effectiveMu ) * _meanUpdate[d];
   _conjugateEvolutionPathL2Norm += std::pow(_evolutionPath[d],2);
 }
 _conjugateEvolutionPathL2Norm = std::sqrt(_conjugateEvolutionPathL2Norm);

 /* update stored paths */
 if ( (_k->_currentGeneration-1) % _setUpdateInterval == 0)
 {
    updateSet();
    updateInverseVectors();
 }

 /* update sigma */
 updateSigma();

 /* numerical error management */
 numericalErrorTreatment();

}


void korali::solver::LMCMAES::choleskyFactorUpdate(size_t sampleIdx)
{
 
  /* randomly select subsetStartIndex */
  double ms = 4.0;
  if (sampleIdx == 0) ms *= 10;
  size_t subsetStartIndex = _subsetSize - std::min( (size_t) std::floor(ms*std::abs(_normalGenerator->getRandomNumber()))+1, _subsetSize);

  //for (size_t d = 0; d < _k->_variables.size(); ++d) _randomVector[d] = (_uniformGenerator->getRandomNumber() > 0.5) ? 1 : -1.0; Rademacher
  if ( _randomNumberDistribution == "Normal") 
      for (size_t d = 0; d < _k->_variables.size(); ++d) _randomVector[d] = _normalGenerator->getRandomNumber();
  else if ( _randomNumberDistribution == "Uniform") 
      for (size_t d = 0; d < _k->_variables.size(); ++d) _randomVector[d] = 2*_uniformGenerator->getRandomNumber()-1.0;

  for (size_t d = 0; d < _k->_variables.size(); ++d) _choleskyFactorVectorProduct[d] = _randomVector[d];
  
  for(size_t i = subsetStartIndex; i < _subsetSize; ++i)
  {
    size_t idx = _subsetHistory[i];
    
    double k = 0.0;
    for(size_t d = 0; d < _k->_variables.size(); ++d) k += _inverseVectors[idx][d] * _randomVector[d];
    k *= _evolutionPathWeights[idx];
    
    for(size_t d = 0; d < _k->_variables.size(); ++d) _choleskyFactorVectorProduct[d] = _sqrtInverseCholeskyRate * _choleskyFactorVectorProduct[d] + k * _evolutionPathHistory[idx][d];
  }
}


void korali::solver::LMCMAES::updateSet()
{
  size_t t = std::floor( double(_k->_currentGeneration-1.0)/ double(_setUpdateInterval) );

  if (t < _subsetSize)
  {
    _replacementIndex     = t;
    _subsetHistory[t]     = t;
    _subsetUpdateTimes[t] = t * _setUpdateInterval + 1;
  }
  else
  {
    double tmparg, minarg, target;
    minarg = std::numeric_limits<double>::max();
    for(size_t i = 1; i < _subsetSize; ++i) 
    { 
      /* `target` by default equals  _k->_variables.size() */
      target = _targetDistanceCoefficients[0] + _targetDistanceCoefficients[1] * std::pow(double(i+1.)/double(_subsetSize), _targetDistanceCoefficients[2]);
      tmparg = _subsetUpdateTimes[_subsetHistory[i]] - _subsetUpdateTimes[_subsetHistory[i-1]] - target;
      if(tmparg < minarg) { minarg = tmparg; _replacementIndex = i; }
    }
    if (tmparg > 0) _replacementIndex = 0; /* if all evolution paths at a distance of `target` or larger, update oldest */
    size_t jtmp = _subsetHistory[_replacementIndex];
    for(size_t i = _replacementIndex; i < _subsetSize-1; ++i) _subsetHistory[i] = _subsetHistory[i+1];
    
    _subsetHistory[_subsetSize-1] = jtmp;
    _subsetUpdateTimes[jtmp]      = t * _setUpdateInterval + 1;
  }
  
  /* insert new evolution path */
  std::copy(_evolutionPath.begin(), _evolutionPath.end(), _evolutionPathHistory[_subsetHistory[_replacementIndex]].begin()); 
  
}


void korali::solver::LMCMAES::updateInverseVectors()
{
  double djt, k;
  double fac = std::sqrt(1.0+_choleskyMatrixLearningRate/(1.0-_choleskyMatrixLearningRate));
  
  /* update all inverse vectors and evolution path weights onwards from replacement index */
  for(size_t i = _replacementIndex; i < _subsetSize; ++i)
  {
    size_t idx = _subsetHistory[i];
    
    double v2L2 = 0.0;
    for(size_t d = 0; d < _k->_variables.size(); ++d) v2L2 += _inverseVectors[idx][d] * _inverseVectors[idx][d];
    
    k = 0.0;
    if (v2L2 > 0.0)
    {
      djt = _sqrtInverseCholeskyRate/v2L2 * (1.0 - 1.0/(fac*std::sqrt(v2L2)));
    
      k = 0.0;
      for(size_t d = 0; d < _k->_variables.size(); ++d) k += _inverseVectors[idx][d] * _evolutionPathHistory[idx][d];
      k *= djt;

      _evolutionPathWeights[idx] = _sqrtInverseCholeskyRate/v2L2 * (std::sqrt(1.0+_choleskyMatrixLearningRate/(1.0-_choleskyMatrixLearningRate)*v2L2) - 1.0);
 
    }
    
    for(size_t d = 0; d < _k->_variables.size(); ++d)
      _inverseVectors[idx][d] = _sqrtInverseCholeskyRate * _evolutionPathHistory[idx][d] - k * _inverseVectors[idx][d];

 }

}


void korali::solver::LMCMAES::updateSigma()
{

 /* update sigma */
 if(_sigmaUpdateRule == "LMCMA")
 {
 double rank = 0;
 for(size_t i = 0; i < _populationSize; ++i)
 {
  if(_valueVector[_sortingIndex[i]] > _previousValueVector[_previousSortingIndex[i]])
  {
   rank++;
   for(size_t j = i+1; (j < _populationSize) && (_valueVector[_sortingIndex[j]] > _previousValueVector[_previousSortingIndex[i]]); ++j) rank++;
  }
  else
  {
   rank--;
   for(size_t j = i+1; (j < _populationSize) && (_previousValueVector[_previousSortingIndex[j]] > _valueVector[_sortingIndex[i]]); ++j) rank--;
  }
 }

 rank = rank/(_populationSize*_populationSize) - _targetSuccessRate;
 _sigmaExponentFactor = (1.0 - _sigmaCumulationFactor) * _sigmaExponentFactor + _sigmaCumulationFactor * rank;
  
   _sigma *= std::exp(_sigmaExponentFactor/_dampFactor); 
 }
 else /* CMA-ES update rule */
 {
   _sigma *= exp(_sigmaCumulationFactor/_dampFactor*(_conjugateEvolutionPathL2Norm/_chiSquareNumber-1.));
 }

 /* escape flat evaluation */
 if (_currentBestValue == _valueVector[_sortingIndex[(int)_muValue]]) {
   _sigma *= exp(0.2+_sigmaCumulationFactor/_dampFactor);
   _k->_logger->logWarning("Detailed", "Sigma increased due to equal function values.\n");
 }

 /* upper bound check for _sigma */
 if(_sigma > 2.0*_initialSigma)
 {
  _k->_logger->logInfo("Detailed", "Sigma exceeding initial sigma by a factor of two (%f > %f), increase value of Initial Sigma.\n", _sigma, 2.0*_initialSigma);
  if( _isSigmaBounded )
  {
    _sigma = 2.0*_initialSigma;
    _k->_logger->logInfo("Detailed", "Sigma set to upper bound (%f) due to solver configuration 'Is Sigma Bounded' = 'true'.\n", _sigma);
  }
 }

}


void korali::solver::LMCMAES::numericalErrorTreatment()
{
 //treat numerical precision provblems
 //TODO
}


/************************************************************************/
/*                    Additional Methods                                */
/************************************************************************/


void korali::solver::LMCMAES::sort_index(const std::vector<double>& vec, std::vector<size_t>& sortingIndex) const
{
  // initialize original _sortingIndex locations
  std::iota(std::begin(sortingIndex), std::end(sortingIndex), (size_t) 0);

  // sort indexes based on comparing values in vec
  std::sort(std::begin(sortingIndex), std::end(sortingIndex), [vec](size_t i1, size_t i2) { return vec[i1] > vec[i2]; } );

}


void korali::solver::LMCMAES::printGenerationBefore() { return; }

void korali::solver::LMCMAES::printGenerationAfter()
{

 _k->_logger->logInfo("Normal", "Sigma:                        %+6.3e\n", _sigma);
 _k->_logger->logInfo("Normal", "Current Function Value: Max = %+6.3e - Best = %+6.3e\n", _currentBestValue, _bestEverValue);

  _k->_logger->logInfo("Detailed", "Variable = (MeanX, BestX):\n");
  for (size_t d = 0; d < _k->_variables.size(); d++) _k->_logger->logData("Detailed", "         %s = (%+6.3e, %+6.3e)\n", _k->_variables[d]->_name.c_str(), _currentMean[d], _bestEverVariables[d]);

  _k->_logger->logInfo("Detailed", "Covariance Matrix:\n");
  for (size_t d = 0; d < _k->_variables.size(); d++)
  {
   //for (size_t e = 0; e <= d; e++) _k->_logger->logData("Detailed", "   %+6.3e  ",_covarianceMatrix[d*_k->_variables.size()+e]);
   _k->_logger->logInfo("Detailed", "\n");
  }

  _k->_logger->logInfo("Detailed", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}

void korali::solver::LMCMAES::finalize()
{
  _k->_logger->logInfo("Minimal", "Optimum found: %e\n", _bestEverValue);
  _k->_logger->logInfo("Minimal", "Optimum found at:\n");
  for (size_t d = 0; d < _k->_variables.size(); ++d) _k->_logger->logData("Minimal", "         %s = %+6.3e\n", _k->_variables[d]->_name.c_str(), _bestEverVariables[d]);
  _k->_logger->logInfo("Detailed", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}
