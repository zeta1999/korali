#include "modules/solver/learner/QTable/QTable.hpp"
#include "math.h"

/////// Mandatory Functions for Learner Interface

void korali::solver::learner::QTable::initialize()
{
 for (size_t i = 0; i < _k->_variables.size(); i++)
  if (_k->_variables[i]->_parameterSpace == "Continuous")
   _k->_logger->logError("Variable %lu (%s) is continuous, but Q-Table based algorithms can only process discrete or custom defined parameter spaces.\n", i, _k->_variables[i]->_name.c_str());
}

void korali::solver::learner::QTable::setInitialConfiguration()
{
 _q.resize(_problem->_actionCount * _problem->_stateCount);

  for (size_t i = 0; i < _q.size(); i++)
   _q[i] = _initialQValue;
}

std::vector<double> korali::solver::learner::QTable::getAction(const std::vector<double>& state)
{
 // Calculating state's index
 size_t stateIdx = getInterpolatedIndex(state, _problem->_stateVectorIndexes);

 return getPolicyAction(stateIdx);
}

void korali::solver::learner::QTable::storeExperience(const std::vector<double>& state, const std::vector<double>& action, const double reward, const std::vector<double>& newState)
{
 // Calculating state's index
 size_t stateIdx = getInterpolatedIndex(state, _problem->_stateVectorIndexes);

 // Creating storage for new action
 std::vector<double> newAction;

 // Q-Learning: Calculating new action with argmax(a): q(s',a)
 if (_qUpdateAlgorithm == "Q-Learning")
  newAction = getBestAction(stateIdx);

 // SARSA: Calculating new action based on policy a: q(s',a -> epsilon-greedy)
 if (_qUpdateAlgorithm == "SARSA")
  newAction = getPolicyAction(stateIdx);

 // Obtaining new state and action indexes
 size_t actionIdx    = getInterpolatedIndex(action,    _problem->_actionVectorIndexes);
 size_t newStateIdx  = getInterpolatedIndex(newState,  _problem->_stateVectorIndexes);
 size_t newActionIdx = getInterpolatedIndex(newAction, _problem->_actionVectorIndexes);

 // Calculating Q indexes
 size_t curQIdx = stateIdx * _problem->_actionCount + actionIdx;
 size_t newQIdx = newStateIdx * _problem->_actionCount + newActionIdx;

 // Updating Q Table
 double curQValue = _q[curQIdx];
 double newQValue = (1 - _learningRate) * curQValue + _learningRate * (reward + _discountFactor * _q[newQIdx]);
 _q[curQIdx] = newQValue;

 // Adding difference to convergence rate
 double diff = abs(newQValue - curQValue);
 if (std::isnan(diff) == false) _convergenceRate += diff;
}

void korali::solver::learner::QTable::initializeGeneration()
{
 // Initializing covergence rate to zero
 _convergenceRate = 0.0;
}

void korali::solver::learner::QTable::processGeneration()
{
 // Adjusting the value of epsilon
 _epsilon = _epsilon - _epsilonDecreaseRate;
 if (_epsilon < 0) _epsilon = 0;
}


/////// Auxiliary functions for QTable

std::vector<double> korali::solver::learner::QTable::getPolicyAction(const size_t& stateIdx)
{
 // Declaring the action vector
 std::vector<double> action;

 // Getting p = U[0,1] for the epsilon strategy
 double p = _uniformGenerator->getRandomNumber();

 // If p < e, then we choose the action randomly, with a uniform probability, among all possible actions.
 if (p < _epsilon) action = getRandomAction();

 // Else, we go for the greedy approach of selecting the best known action for the state.
 else action = getBestAction(stateIdx);

 return action;
}

std::vector<double> korali::solver::learner::QTable::getBestAction(const size_t& stateIdx)
{
 // Setting initial and final positions of the Q-Table
 size_t startPos = stateIdx * _problem->_actionCount;
 size_t finalPos = startPos + _problem->_actionCount;

 // Initializing best Q value with minus infinity
 double qVal = -korali::Inf;
 size_t qIdx = 0;

 // Iterating over the current state's entries of the Q table
 for (size_t pos = startPos; pos < finalPos; pos++)
  if (_q[pos] > qVal) { qVal = _q[pos]; qIdx = pos; }

 // Obtaining action index
 size_t actionIdx = qIdx - startPos;

 // Returning action
 return getActionFromIndex(actionIdx);
}

std::vector<double> korali::solver::learner::QTable::getRandomAction()
{
 // Getting p = U[0,1] for the epsilon strategy
 double p = _uniformGenerator->getRandomNumber();

 // Getting random index for action
 double idxSpace = _problem->_actionCount;

 // We use the floor function to get the integer index of the selected action
 size_t actionIdx = floor(p * idxSpace);

 return getActionFromIndex(actionIdx);
}

std::vector<double> korali::solver::learner::QTable::getActionFromIndex(const size_t& actionIdx)
{
 // Creating storage for new action
 std::vector<double> action(_problem->_actionVectorSize);

 size_t tempIdx = actionIdx;
 for (size_t i = 0; i < _problem->_actionVectorSize; i++)
 {
  size_t varIdx = _problem->_actionVectorIndexes[i];
  size_t varParVectorSize = _k->_variables[varIdx]->_parameterVector.size();
  size_t parameterIdx = tempIdx % varParVectorSize;
  action[i] = _k->_variables[varIdx]->_parameterVector[parameterIdx];
  tempIdx = tempIdx / varParVectorSize;
 }

 return action;
}

size_t korali::solver::learner::QTable::getInterpolatedIndex(const std::vector<double>& values, const std::vector<size_t>& indexes)
{
 size_t pos = 0;
 size_t multiplier = 1;

 for (size_t i = 0; i < values.size(); i++)
 {
  double value = values[i];
  size_t index = indexes[i];

  double minDiff = korali::Inf;
  size_t valPos = 0;
  for (size_t j = 0; j < _k->_variables[index]->_parameterVector.size(); j++)
  {
   double diff = abs(value - _k->_variables[index]->_parameterVector[j]);
   if (diff < minDiff) { minDiff = diff; valPos = j; }
  }

  pos += valPos * multiplier;
  multiplier *= _k->_variables[index]->_parameterVector.size();
 }

 return pos;
}


/////// Output functions for QTable

void korali::solver::learner::QTable::printGenerationBefore()
{

}

void korali::solver::learner::QTable::printGenerationAfter()
{
 _k->_logger->logInfo("Normal", "Best Reward: %f\n", _bestReward);
 _k->_logger->logInfo("Normal", "Convergence Rate: %f\n", _convergenceRate);
}

