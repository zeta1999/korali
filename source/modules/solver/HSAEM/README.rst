*********************************************
SAEM Adjusted for Hierarchical Models
*********************************************

The Stochastic Approximation Expectation Maximization (SAEM) solver is targeted at
a very general form of latent variable problems. If, however, a problem can be brought
into a hierarchical form with transformed-normal prior :math:`q`:

.. math::

  p(X, \theta | \psi) = p(X | \theta) \cdot q(\theta | \psi),

then a more specialized solver can be used - this one, `HSAEM`. This can simplify experiment setup considerably.

In the above formula,
 -  :math:`X = (x_i)_i` are observed data points
    (entirely dealt with by the user, only mentioned for clarity)
 -  :math:`\theta` are latent (i.e. unobserved) variables,
 -  :math:`\psi` are hyperparameters.

:math:`p(X | \theta )` **is given by the user** and can have arbitrary an form
(that is a - possibly unnormalized - probability distribution, i.e. it should have
a finite integral over :math:`X`).

:math:`q(\theta | \psi )` is **generated by Korali**. Each latent variable in :math:`\theta`
can be chosen to have a **normal**, **log-normal** or **logit-normal** distribution.
(Log-normal and logit-normal distributions refer to the log or logit of
:math:`\theta_i` being normally distributed.)


As opposed to SAEM, HSAEM implements sampling for the E-step internally, so no sampler
needs to be given by the user. In addition, it comes with an option for Simulated Annealing,
as shortly described below.

We exactly follow the method described in section 9.2.4.1 in `the book by Lavielle <http://www.cmap.polytechnique.fr/~lavielle/book.html>`_.



Examples
--------

Four examples how to use this solver can be found in folder `examples/hierarchical.bayesian/latent.variables`:

- `run-saem-hierarchical.py`,
- `run-saem-hierarchical-nd.py`,
- `run-saem-normal.py` and
- `run-saem-logistic.py`.



Parameters that influence runtime
---------------------------------
- The number of MCMC steps per generation is (`"Number Samples Per Step"`) *
  (`"N1"` + `"N2"` + `"N3"`) * (`"mcmc Outer Steps"`).
  If HSAEM takes too long, check whether you can reduce this number without impacting accuracy.
- Other parameters influence how many iterations are needed until convergence:

  - If convergence is slow, try reducing `"Alpha 1"`, while keeping
    sufficiently many `"Number Initial Steps"` during which `"Alpha 1"`
    is active.
  - Note: `"Alpha 1"` is only active in genrations `"K1"` to  `"Number Initial Steps"`.
  - Simulated Annealing (SA): Increasing the initial SA variance and
    decreasing its decay factor will increase stochasticity.
  - Note: SA only has an effect if the SA variance is larger than some of the
    diagonal entries of the estimated covariance matrix.


Simulated Annealing
-------------------
Simulated Annealing (SA, see `Kirkpatrick 1984 <https://link.springer.com/article/10.1007/BF01009452>`_ ) is a technique
for stochastic optimization methods, where the probability distribution is widened artificially, by a factor decreasing
with iterations, to allow more exploration and prevent getting stuck in local minima.

Parameters that control SA in the SAEM solver are:

.. code-block:: python

  # Setup before setting parameters
  k = korali.Engine()
  e = korali.Experiment()
  e["Solver"]["Type"] = "HSAEM"

  # First we need to enable it:
  e["Solver"]["Use Simulated Annealing" ] = true
  # Ka: Number of iterations during which SA is active.
  e["Solver"]["Ka"] = 200
  # SA works by replacing the sampling variance by something larger. So to let
  #  SA do its job, make sure this variance is larger than what it replaces:
  e["Solver"]["Simulated Annealing Initial Variance"] = 5.0
  # The larger variance from SA will be decreased each iteration by:
  e["Solver"]["Simulated Annealing Decay Factor"] = 0.95


Korali's implementation of SA for SAEM follows the proposal in chapter 9.2.6 in the `book by Lavielle <http://www.cmap.polytechnique.fr/~lavielle/book.html>`_ .



