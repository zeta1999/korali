#include "modules/solver/Rprop/Rprop.hpp"
#include "modules/problem/problem.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/conduit/conduit.hpp"

#include <stdio.h>

void korali::solver::Rprop::setInitialConfiguration()
{
  for (size_t i = 0; i < _k->_variables.size(); i++)
    if( std::isfinite(_k->_variables[i]->_initialValue) == false )
      korali::logError("Initial Value of variable \'%s\' not defined (no defaults can be calculated).\n", _k->_variables[i]->_name.c_str());

  _currentX.resize( _k->_variables.size(), 0.0 );
  for (size_t i = 0; i < _k->_variables.size(); i++)
    _currentX[i] = _k->_variables[i]->_initialValue;

  _bestX = _currentX;
  _delta.resize( _k->_variables.size(), _delta0 );
  _currentGradient.resize( _k->_variables.size(), 0);
  _previousGradient.resize( _k->_variables.size(), 0.0 );

  _bestEvaluation = korali::Inf;
  _xDiff = korali::Inf;
  _maxStallCounter = 0;
  _normPreviousGradient = korali::Inf;
  _previousEvaluation   = korali::Inf;
}

void korali::solver::Rprop::evaluateFunctionAndGradient( korali::Sample& sample )
{
  // Initializing Sample Evaluation
  sample["Operation"]  = "Evaluate";
  sample["Parameters"] = _currentX;
  sample["Sample Id"]  = 0;
  _modelEvaluationCount++;
  korali::_conduit->start(sample);

  // Waiting for samples to finish
  korali::_conduit->wait(sample);

  // Processing results
  // The 'minus' is there because we want Rprop to do Maximization be default.
  _currentEvaluation = sample["F(x)"];
  _currentEvaluation = -_currentEvaluation;
  for( size_t i=0; i<_k->_variables.size(); i++)
  {
    _currentGradient[i] = sample["Gradient"][i];
    _currentGradient[i] = -_currentGradient[i];
  }
}

void korali::solver::Rprop::runGeneration( void )
{
  if (_k->_currentGeneration == 1) setInitialConfiguration();

  korali::Sample sample;

  evaluateFunctionAndGradient(sample);

  korali::logInfo("Normal","X = [ ");
  for(size_t k=0; k<_k->_variables.size(); k++) korali::logData("Normal"," %.5le  ",_currentX[k]);
  korali::logData("Normal"," ]\n");

  korali::logInfo("Normal","F(X) = %le \n", _currentEvaluation );

  korali::logInfo("Normal","DF(X) = [ ");
  for(size_t k=0; k<_k->_variables.size(); k++) korali::logData("Normal"," %.5le  ",_currentGradient[k]);
  korali::logData("Normal"," ]\n");

  korali::logInfo("Normal","X_best = [ ");
  for(size_t k=0; k<_k->_variables.size(); k++) korali::logData("Normal"," %.5le  ",_bestX[k]);
  korali::logData("Normal"," ]\n");

  Update_iminus();

  _previousEvaluation   = _currentEvaluation;
  _previousGradient     = _currentGradient;
  _normPreviousGradient = vectorNorm(_previousGradient);

  if( _currentEvaluation < _bestEvaluation ){
    _bestEvaluation = _currentEvaluation;
    (*_k)["Results"]["Best Sample"] = sample._js.getJson();

    std::vector<double> tmp(_k->_variables.size());
    for(size_t j=0; j<_k->_variables.size(); j++) tmp[j] = _bestX[j]-_currentX[j];
    _xDiff = vectorNorm( tmp );
    _bestX = _currentX;
    _maxStallCounter = 0;
  }
  else{
    _maxStallCounter++;
  }
}

// Rprop_plus
void korali::solver::Rprop::Update_plus( void ){
  for(size_t i=0; i<_k->_variables.size(); i++){
    double productGradient = _previousGradient[i] * _currentGradient[i];
    if( productGradient > 0 ){
      _delta[i] = std::min( _delta[i]*_etaPlus, _deltaMax );
      _currentGradient[i] = -sign(_currentGradient[i]) * _delta[i];
      _currentX[i] += _currentGradient[i];
    }
    else if( productGradient < 0 ){
      _delta[i] = std::max( _delta[i]*_etaMinus, _deltaMin );
      _currentX[i] -= _currentGradient[i];
      _currentGradient[i] = 0;
    }
    else{
      _currentGradient[i] = -sign(_currentGradient[i]) * _delta[i];
      _currentX[i] += _currentGradient[i];
    }
  }
}

// Rprop_minus
void korali::solver::Rprop::Update_minus( void ){
  for(size_t i=0; i<_k->_variables.size(); i++){
    double productGradient = _previousGradient[i] * _currentGradient[i];
    if( productGradient > 0 )
      _delta[i] = std::min( _delta[i]*_etaPlus, _deltaMax );
    else if( productGradient < 0 )
      _delta[i] = std::max( _delta[i]*_etaMinus, _deltaMin );
    _currentX[i] += -sign(_currentGradient[i]) * _delta[i];
  }
}

// iRprop_plus
void korali::solver::Rprop::Update_iplus( void ){
  for(size_t i=0; i<_k->_variables.size(); i++){
    double productGradient = _previousGradient[i] * _currentGradient[i];
    if( productGradient > 0 ){
      _delta[i] = std::min( _delta[i]*_etaPlus, _deltaMax );
      _currentGradient[i] = -sign(_currentGradient[i]) * _delta[i];
      _currentX[i] += _currentGradient[i];
    }
    else if( productGradient < 0 ){
      _delta[i] = std::max( _delta[i]*_etaMinus, _deltaMin );
      if( _currentEvaluation > _previousEvaluation ) _currentX[i] -= _currentGradient[i];
      _currentGradient[i] = 0;
    }
    else{
      _currentGradient[i] = -sign(_currentGradient[i]) * _delta[i];
      _currentX[i] += _currentGradient[i];
    }
  }
}

// iRprop_minus
void korali::solver::Rprop::Update_iminus( void ){
  for(size_t i=0; i<_k->_variables.size(); i++){
    double productGradient = _previousGradient[i] * _currentGradient[i];
    if( productGradient > 0 ){
      _delta[i] = std::min( _delta[i]*_etaPlus, _deltaMax );
    }
    else if( productGradient < 0 ){
      _delta[i] = std::max( _delta[i]*_etaMinus, _deltaMin );
      _currentGradient[i] = 0;
    }
    _currentX[i] += -sign(_currentGradient[i]) * _delta[i];
  }
}

void korali::solver::Rprop::printGenerationBefore(){ return; }

void korali::solver::Rprop::printGenerationAfter() { return; }

void korali::solver::Rprop::finalize()  { return; }
