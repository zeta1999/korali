#include "modules/conduit/conduit.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/problem/problem.hpp"
#include "modules/solver/Rprop/Rprop.hpp"

#include <stdio.h>

void korali::solver::Rprop::setInitialConfiguration()
{
  for (size_t i = 0; i < _k->_variables.size(); i++)
    if (std::isfinite(_k->_variables[i]->_initialValue) == false)
      korali::Logger::logError("Initial Value of variable \'%s\' not defined (no defaults can be calculated).\n", _k->_variables[i]->_name.c_str());

  _currentX.resize(_k->_variables.size(), 0.0);
  for (size_t i = 0; i < _k->_variables.size(); i++)
    _currentX[i] = _k->_variables[i]->_initialValue;

  _bestX = _currentX;
  _delta.resize(_k->_variables.size(), _delta0);
  _currentGradient.resize(_k->_variables.size(), 0);
  _previousGradient.resize(_k->_variables.size(), 0.0);

  _bestEvaluation = korali::Inf;
  _xDiff = korali::Inf;
  _maxStallCounter = 0;
  _normPreviousGradient = korali::Inf;
  _previousEvaluation = korali::Inf;
}

void korali::solver::Rprop::evaluateFunctionAndGradient(korali::Sample &sample)
{
  // Initializing Sample Evaluation
  sample["Module"] = "Problem";
  sample["Operation"] = "Evaluate";
  sample["Parameters"] = _currentX;
  sample["Sample Id"] = 0;
  _modelEvaluationCount++;
  _conduit->start(sample);

  // Waiting for samples to finish
  _conduit->wait(sample);

  // Processing results
  // The 'minus' is there because we want Rprop to do Maximization be default.
  try
  {
    _currentEvaluation = sample["F(x)"];
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Rprop: Missing or incorrect value of 'F(x)' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its result, e.g., sample[\"F(x)\"] = value.\n   + Cause: %s\n", e.what());
  }

  _currentEvaluation = -_currentEvaluation;
  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
    try
    {
      _currentGradient[i] = sample["Gradient"][i];
    }
    catch (const std::exception &e)
    {
      korali::Logger::logError("Rprop: Missing, incomplete, or incorrect value of the gradient returned by the sample evaluation on position %lu. \n   + Solution: Make sure your model is storing its result, e.g., sample[\"Gradient(x)\"] = value(s).\n   + Cause: %s\n", i, e.what());
    }

    _currentGradient[i] = -_currentGradient[i];
  }
}

void korali::solver::Rprop::runGeneration(void)
{
  if (_k->_currentGeneration == 1) setInitialConfiguration();

  korali::Sample sample;

  evaluateFunctionAndGradient(sample);

  _k->_logger->logInfo("Normal", "X = [ ");
  for (size_t k = 0; k < _k->_variables.size(); k++) _k->_logger->logData("Normal", " %.5le  ", _currentX[k]);
  _k->_logger->logData("Normal", " ]\n");

  _k->_logger->logInfo("Normal", "F(X) = %le \n", _currentEvaluation);

  _k->_logger->logInfo("Normal", "DF(X) = [ ");
  for (size_t k = 0; k < _k->_variables.size(); k++) _k->_logger->logData("Normal", " %.5le  ", _currentGradient[k]);
  _k->_logger->logData("Normal", " ]\n");

  _k->_logger->logInfo("Normal", "X_best = [ ");
  for (size_t k = 0; k < _k->_variables.size(); k++) _k->_logger->logData("Normal", " %.5le  ", _bestX[k]);
  _k->_logger->logData("Normal", " ]\n");

  Update_iminus();

  _previousEvaluation = _currentEvaluation;
  _previousGradient = _currentGradient;
  _normPreviousGradient = vectorNorm(_previousGradient);

  if (_currentEvaluation < _bestEvaluation)
  {
    _bestEvaluation = _currentEvaluation;
    (*_k)["Results"]["Best Sample"] = sample._js.getJson();

    std::vector<double> tmp(_k->_variables.size());
    for (size_t j = 0; j < _k->_variables.size(); j++) tmp[j] = _bestX[j] - _currentX[j];
    _xDiff = vectorNorm(tmp);
    _bestX = _currentX;
    _maxStallCounter = 0;
  }
  else
  {
    _maxStallCounter++;
  }
}

// Rprop_plus
void korali::solver::Rprop::Update_plus(void)
{
  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
    double productGradient = _previousGradient[i] * _currentGradient[i];
    if (productGradient > 0)
    {
      _delta[i] = std::min(_delta[i] * _etaPlus, _deltaMax);
      _currentGradient[i] = -sign(_currentGradient[i]) * _delta[i];
      _currentX[i] += _currentGradient[i];
    }
    else if (productGradient < 0)
    {
      _delta[i] = std::max(_delta[i] * _etaMinus, _deltaMin);
      _currentX[i] -= _currentGradient[i];
      _currentGradient[i] = 0;
    }
    else
    {
      _currentGradient[i] = -sign(_currentGradient[i]) * _delta[i];
      _currentX[i] += _currentGradient[i];
    }
  }
}

// Rprop_minus
void korali::solver::Rprop::Update_minus(void)
{
  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
    double productGradient = _previousGradient[i] * _currentGradient[i];
    if (productGradient > 0)
      _delta[i] = std::min(_delta[i] * _etaPlus, _deltaMax);
    else if (productGradient < 0)
      _delta[i] = std::max(_delta[i] * _etaMinus, _deltaMin);
    _currentX[i] += -sign(_currentGradient[i]) * _delta[i];
  }
}

// iRprop_plus
void korali::solver::Rprop::Update_iplus(void)
{
  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
    double productGradient = _previousGradient[i] * _currentGradient[i];
    if (productGradient > 0)
    {
      _delta[i] = std::min(_delta[i] * _etaPlus, _deltaMax);
      _currentGradient[i] = -sign(_currentGradient[i]) * _delta[i];
      _currentX[i] += _currentGradient[i];
    }
    else if (productGradient < 0)
    {
      _delta[i] = std::max(_delta[i] * _etaMinus, _deltaMin);
      if (_currentEvaluation > _previousEvaluation) _currentX[i] -= _currentGradient[i];
      _currentGradient[i] = 0;
    }
    else
    {
      _currentGradient[i] = -sign(_currentGradient[i]) * _delta[i];
      _currentX[i] += _currentGradient[i];
    }
  }
}

// iRprop_minus
void korali::solver::Rprop::Update_iminus(void)
{
  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
    double productGradient = _previousGradient[i] * _currentGradient[i];
    if (productGradient > 0)
    {
      _delta[i] = std::min(_delta[i] * _etaPlus, _deltaMax);
    }
    else if (productGradient < 0)
    {
      _delta[i] = std::max(_delta[i] * _etaMinus, _deltaMin);
      _currentGradient[i] = 0;
    }
    _currentX[i] += -sign(_currentGradient[i]) * _delta[i];
  }
}

void korali::solver::Rprop::printGenerationBefore() { return; }

void korali::solver::Rprop::printGenerationAfter() { return; }

void korali::solver::Rprop::finalize() { return; }
