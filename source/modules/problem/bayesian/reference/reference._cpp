#include <gsl/gsl_sf_gamma.h>
#include "modules/problem/bayesian/reference/reference.hpp"
#include "modules/conduit/conduit.hpp"
#include "modules/experiment/experiment.hpp"

void korali::problem::bayesian::Reference::initialize()
{
  korali::problem::Bayesian::initialize();

  if (_referenceData.size() == 0) _k->_logger->logError("Bayesian (%s) problems require defining reference data.\n", _likelihoodModel.c_str());

  if( _likelihoodModel=="Additive Normal" || _likelihoodModel=="Additive Normal Variance" || _likelihoodModel=="Multiplicative Normal" || _likelihoodModel=="Multiplicative Normal Data"){
    bool foundSigma = false;
    for (size_t i = 0; i < _k->_variables.size(); i++){
      std::string varName = _k->_variables[i]->_name;
      if (varName == "[Sigma]") { sigmaVariableIndex = i; foundSigma = true; }
    }
    if (foundSigma == false) _k->_logger->logError("Bayesian (%s) problems require defining a variable named: '[Sigma]'.\n", _likelihoodModel.c_str());
  }

  std::string str ("Negative Binomial");
  _negativeBinomialConstant = korali::NaN;
  if( _likelihoodModel.find(str) != std::string::npos ){
    _negativeBinomialConstant = 0.;
    for( size_t i=0; i<_referenceData.size(); i++ ){
      if( _referenceData[i]<0 ) _k->_logger->logError("Reference data must be greater or equal to zero for likelihood model: %s\n",_likelihoodModel.c_str());
      _negativeBinomialConstant -= gsl_sf_lngamma( _referenceData[i]+1. );
    }
  }

  if (_k->_variables.size() < 1) _k->_logger->logError("Bayesian (%s) inference problems require at least one variable.\n", _likelihoodModel.c_str());
}


void korali::problem::bayesian::Reference::evaluateLogLikelihood(korali::Sample& sample)
{
  sample.run(_computationalModel);

  if (sample["Reference Evaluations"].size() != _referenceData.size())
    _k->_logger->logError("This Bayesian (%s) problem requires a %lu-sized result array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), sample["Reference Evaluations"].size());

  if (_likelihoodModel == "Additive Normal")            loglikelihoodNormalAdditive(sample);
  if (_likelihoodModel == "Additive Normal Variance")   loglikelihoodNormalAdditiveVariance(sample);
  if (_likelihoodModel == "Multiplicative Normal")      loglikelihoodNormalMultiplicative(sample);
  if (_likelihoodModel == "Multiplicative Normal Data") loglikelihoodNormalMultiplicativeData(sample);

  if (_likelihoodModel == "Additive Normal General" ){
    if( sample["Reference Evaluations"].size() != sample["Standard Deviation Model"].size() )
      _k->_logger->logError("This Bayesian (%s) problem requires two %lu-sized result arrays. Provided: %lu and %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), sample["Reference Evaluations"].size(), sample["Standard Deviation Model"].size() );
    loglikelihoodNormalGeneral(sample);
  }

  if (_likelihoodModel == "Negative Binomial General" ){
    if( sample["Reference Evaluations"].size() != sample["Dispersion"].size() )
      _k->_logger->logError("This Bayesian (%s) problem requires two %lu-sized result arrays. Provided: %lu and %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), sample["Reference Evaluations"].size(), sample["Dispersion"].size() );
    loglikelihoodNegativeBinomialGeneral(sample);
  }

}


double korali::problem::bayesian::Reference::compute_sse( std::vector<double> x, std::vector<double> y ){
  double sse = 0.;
  for(size_t i = 0; i < y.size(); i++)
  {
    if( !isfinite(x[i]) )
    {
      _k->_logger->logWarning("Normal","Non-finite value detected in the results passed in the log-likelihood function.\n");
      return korali::Inf;
    }
    double diff = y[i] - x[i];
    sse += diff*diff;
  }
  return sse;
}


double korali::problem::bayesian::Reference::compute_normalized_sse( std::vector<double> f, std::vector<double> g, std::vector<double> y ){
  double sse = 0.;
  for(size_t i = 0; i < y.size(); i++)
  {

    if( !isfinite(f[i]) || !isfinite(g[i]) ){
      _k->_logger->logWarning("Normal","Non-finite value detected in the results passed in the log-likelihood function.\n");
      return korali::Inf;
    }
    if( g[i]<=0 ){
      _k->_logger->logError("Negative or zero value detected in the standard deviation results passed in the log-likelihood function.\n");
      return korali::Inf;
    }
    double diff = ( y[i] - f[i] ) / g[i];
    sse += diff*diff;
  }
  return sse;
}


void korali::problem::bayesian::Reference::loglikelihoodNormalAdditive(korali::Sample& sample)
{
  double sigma   = sample["Parameters"][sigmaVariableIndex];
  double sigma2  = sigma*sigma;

  double sse = compute_sse( sample["Reference Evaluations"], _referenceData );

  if( isinf(sse) )
    sample["logLikelihood"] = -korali::Inf;
  else
    sample["logLikelihood"] = -0.5*( _referenceData.size()*log(2*M_PI*sigma2) + sse/sigma2);
}


void korali::problem::bayesian::Reference::loglikelihoodNormalAdditiveVariance(korali::Sample& sample)
{
  double sigma2 = sample["Parameters"][sigmaVariableIndex];

  double sse = compute_sse( sample["Reference Evaluations"], _referenceData );

  if( isinf(sse) )
    sample["logLikelihood"] = -korali::Inf;
  else
    sample["logLikelihood"] = -0.5*( _referenceData.size()*log(2*M_PI*sigma2) + sse/sigma2);
}


void korali::problem::bayesian::Reference::loglikelihoodNormalMultiplicative(korali::Sample& sample)
{
  double sigma    = sample["Parameters"][sigmaVariableIndex];
  double ssn      = 0.0;
  double logSigma = 0.0;

  for(size_t i = 0; i < _referenceData.size(); i++)
  {
    double eval = sample["Reference Evaluations"][i];

    if( !isfinite(eval) )
    {
      _k->_logger->logWarning("Normal","Non-finite value detected in the results passed in the log-likelihood function.\n");
      sample["logLikelihood"] = -korali::Inf;
      return;
    }

    double diff   = _referenceData[i] - eval;
    double denom  = sigma*eval;
    ssn += diff*diff / (denom*denom);
    logSigma += log(denom);
  }

  sample["logLikelihood"] = -0.5*( _referenceData.size()*log(2*M_PI) + ssn) - _referenceData.size()*logSigma;
}


void korali::problem::bayesian::Reference::loglikelihoodNormalMultiplicativeData(korali::Sample& sample)
{
  double sigma    = sample["Parameters"][sigmaVariableIndex];
  double ssn      = 0.0;
  double logSigma = 0.0;
  for(size_t i = 0; i < _referenceData.size(); i++)
  {
    double eval = sample["Reference Evaluations"][i];

    if( !isfinite(eval) )
    {
      _k->_logger->logWarning("Normal","Non-finite value detected in the results passed in the log-likelihood function.\n");
      sample["logLikelihood"] = -korali::Inf;
      return;
    }

    double diff   = _referenceData[i] - eval;
    double denom  = sigma*_referenceData[i];
    ssn += diff*diff / (denom*denom);
    logSigma += log(denom);
  }

  sample["logLikelihood"] = -0.5*( _referenceData.size()*log(2*M_PI) + ssn) - _referenceData.size()*logSigma;
}


void korali::problem::bayesian::Reference::loglikelihoodNormalGeneral(korali::Sample& sample)
{
  double sse = compute_normalized_sse( sample["Reference Evaluations"], sample["Standard Deviation Model"], _referenceData );

  if( isinf(sse) )
    sample["logLikelihood"] = -korali::Inf;
  else{
     double loglike = 0.;
    for(size_t i=0; i<sample["Standard Deviation Model"].size(); i++)
      loglike -= log( (double) sample["Standard Deviation Model"][i] );
    loglike +=  -0.5*( _referenceData.size() + sse );

    sample["logLikelihood"] = loglike;
  }
}


void korali::problem::bayesian::Reference::loglikelihoodNegativeBinomialGeneral(korali::Sample& sample)
{

  size_t N = _referenceData.size();

  double loglike = _negativeBinomialConstant;

  for( size_t i=0; i<N; i++ ){
    double m = sample["Reference Evaluations"][i];
    double r = sample["Dispersion"][i];
    double p = m/(m+r);
    double y = _referenceData[i];

    loglike += gsl_sf_lngamma( y+r );
    loglike -= gsl_sf_lngamma( r );
    loglike += r*log( 1-p );
    loglike += y*log( p );
  }

  sample["logLikelihood"] = loglike;
}
