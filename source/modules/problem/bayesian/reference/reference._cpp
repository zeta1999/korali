#include "modules/conduit/conduit.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/problem/bayesian/reference/reference.hpp"
#include <gsl/gsl_blas.h>
#include <gsl/gsl_cdf.h>
#include <gsl/gsl_matrix.h>
#include <gsl/gsl_sf_gamma.h>

void korali::problem::bayesian::Reference::initialize()
{
  korali::problem::Bayesian::initialize();

  if (_referenceData.size() == 0) korali::Logger::logError("Bayesian (%s) problems require defining reference data.\n", _likelihoodModel.c_str());

  if (_k->_variables.size() < 1) korali::Logger::logError("Bayesian (%s) inference problems require at least one variable.\n", _likelihoodModel.c_str());
}

void korali::problem::bayesian::Reference::evaluateLoglikelihood(korali::Sample &sample)
{
  sample.run(_computationalModel);

  try
  {
    if (sample["Reference Evaluations"].size() != _referenceData.size())
      korali::Logger::logError("This Bayesian (%s) problem requires a %lu-sized result array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), sample["Reference Evaluations"].size());
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Reference Evaluations' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its reference evaluations, e.g., sample[\"Reference Evaluations\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  if (_likelihoodModel == "Normal")
    loglikelihoodNormal(sample);
  else if (_likelihoodModel == "Negative Binomial")
    loglikelihoodNegativeBinomial(sample);
  else if (_likelihoodModel == "Positive Normal")
    loglikelihoodPositiveNormal(sample);
  else
    korali::Logger::logError("Bayesian problem (%s) not recognized.\n", _likelihoodModel.c_str());
}

double korali::problem::bayesian::Reference::compute_normalized_sse(std::vector<double> f, std::vector<double> g, std::vector<double> y)
{
  double sse = 0.;
  for (size_t i = 0; i < y.size(); i++)
  {
    double diff = (y[i] - f[i]) / g[i];
    sse += diff * diff;
  }
  return sse;
}

void korali::problem::bayesian::Reference::loglikelihoodNormal(korali::Sample &sample)
{
  try
  {
    if (sample["Standard Deviation"].size() != _referenceData.size())
      korali::Logger::logError("This Bayesian (%s) problem requires a %lu-sized Standard Deviation array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), sample["Standard Deviation"].size());
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Standard Deviation' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its standard deviation, e.g., sample[\"Standard Deviation\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  double sse = -korali::Inf;
  try
  {
    sse = compute_normalized_sse(sample["Reference Evaluations"], sample["Standard Deviation"], _referenceData);
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Standard Deviation' or 'Reference Evaluations' returned by the model.\n   + Cause: %s\n", e.what());
  }

  double loglike = 0.;
  double sdev;
  for (size_t i = 0; i < sample["Standard Deviation"].size(); i++)
  {
    try
    {
      sdev = sample["Standard Deviation"][i];
    }
    catch (const std::exception &e)
    {
      korali::Logger::logError("Missing or incorrect value of 'Standard Deviation' at position %lu returned by the sample evaluation. \n   + Solution: Make sure your model is storing its standard deviation, e.g., sample[\"Standard Deviation\"] = value(s).\n   + Cause: %s\n", e.what());
    }

    if (sdev <= 0.0) korali::Logger::logError("Negative or zero value (%lf) detected for the Standard Deviation.\n", sdev);
    loglike -= log(sdev);
  }

  loglike -= 0.5 * (_referenceData.size() * _log2pi + sse);
  sample["logLikelihood"] = loglike;
}

void korali::problem::bayesian::Reference::loglikelihoodPositiveNormal(korali::Sample &sample)
{
  double sse = compute_normalized_sse(sample["Reference Evaluations"], sample["Standard Deviation"], _referenceData);
  double loglike = 0.;
  for (size_t i = 0; i < sample["Standard Deviation"].size(); i++)
  {
    double m = -korali::Inf;
    try
    {
      m = sample["Reference Evaluations"][i];
    }
    catch (const std::exception &e)
    {
      korali::Logger::logError("Missing or incorrect value of 'Reference Evaluations' at position %lu returned by the sample evaluation. \n   + Solution: Make sure your model is storing its reference evaluations, e.g., sample[\"Reference Evaluations\"] = value(s).\n   + Cause: %s\n", i, e.what());
    }

    double s = -korali::Inf;
    try
    {
      s = sample["Standard Deviation"][i];
    }
    catch (const std::exception &e)
    {
      korali::Logger::logError("Missing or incorrect value of 'Standard Deviation' at position %lu returned by the sample evaluation. \n   + Solution: Make sure your model is storing its reference evaluations, e.g., sample[\"Reference Evaluations\"] = value(s).\n   + Cause: %s\n", i, e.what());
    }

    if (s <= 0.0) korali::Logger::logError("Negative or zero value (%lf) detected for the Standard Deviation.\n", s);
    if (m < 0.0) korali::Logger::logError("Negative value (%lf) detected in Reference Evaluation.\n", m);

    loglike -= log(s);
    loglike -= log(1. - gsl_cdf_gaussian_P(-m, s));
  }
  loglike += -0.5 * (_referenceData.size() * _log2pi + sse);
  sample["logLikelihood"] = loglike;
}

void korali::problem::bayesian::Reference::loglikelihoodNegativeBinomial(korali::Sample &sample)
{
  size_t N = _referenceData.size();
  double loglike = 0.0;

  for (size_t i = 0; i < N; i++)
  {
    loglike -= gsl_sf_lngamma(_referenceData[i] + 1.);

    double m = -korali::Inf;
    try
    {
      m = sample["Reference Evaluations"][i];
    }
    catch (const std::exception &e)
    {
      korali::Logger::logError("Missing or incorrect value of 'Reference Evaluations' at position %lu returned by the sample evaluation. \n   + Solution: Make sure your model is storing its reference evaluations, e.g., sample[\"Reference Evaluations\"] = value(s).\n   + Cause: %s\n", i, e.what());
    }
    if (m < 0.0) korali::Logger::logError("Negative value (%lf) detected in Reference Evaluation.\n", m);


    double r = -korali::Inf;
    try
    {
      double r = sample["Dispersion"][i];
    }
    catch (const std::exception &e)
    {
      korali::Logger::logError("Missing or incorrect value of 'Dispersion' at position %lu returned by the sample evaluation. \n   + Solution: Make sure your model is storing its reference evaluations, e.g., sample[\"Dispersion\"] = value(s).\n   + Cause: %s\n", i, e.what());
    }

    if (r <= 0.0) korali::Logger::logError("Negative or zero value (%lf) detected for the Dispersion.\n", r);
    double p = m / (m + r);
    double y = _referenceData[i];

    loglike += gsl_sf_lngamma(y + r);
    loglike -= gsl_sf_lngamma(r);
    loglike += r * log(1 - p);
    loglike += y * log(p);
  }

  sample["logLikelihood"] = loglike;
}

void korali::problem::bayesian::Reference::evaluateLoglikelihoodGradient(korali::Sample &sample)
{
  double eval = sample["F(x)"];
  if (isfinite(eval))
  {
    if (_likelihoodModel == "Normal")
      gradientLoglikelihoodNormal(sample);
    else if (_likelihoodModel == "Positive Normal")
      gradientLoglikelihoodPositiveNormal(sample);
    else if (_likelihoodModel == "Negative Binomial")
      gradientLoglikelihoodNegativeBinomial(sample);
    else
      korali::Logger::logError("Gradient not yet implemented for selected bayesian problem and log likelihood model.");
  }
  else
  {
    sample["logLikelihood Gradient"] = std::vector<double>(_k->_variables.size(), 0.0);
  }
}

void korali::problem::bayesian::Reference::gradientLoglikelihoodNormal(korali::Sample &sample)
{
  std::vector<double> refEvaluations;
  std::vector<double> standardDev;
  std::vector<std::vector<double>> gradientF;
  std::vector<std::vector<double>> gradientG;

  try
  {
    refEvaluations = sample["Reference Evaluations"].get<std::vector<double>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Reference Evaluations' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its reference evaluations, e.g., sample[\"Reference Evaluations\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  try
  {
    standardDev = sample["Standard Deviation"].get<std::vector<double>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Standard Deviation' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its Standard Deviation, e.g., sample[\"Standard Deviation\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  try
  {
    gradientF = sample["Gradient Mean"].get<std::vector<std::vector<double>>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Gradient Mean' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its Gradient Mean, e.g., sample[\"Gradient Mean\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  try
  {
    gradientG = sample["Gradient Standard Deviation"].get<std::vector<std::vector<double>>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Gradient Standard Deviation' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its Gradient Standard Deviation, e.g., sample[\"Gradient Standard Deviation\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  if (sample["Gradient Mean"].size() != refEvaluations.size()) korali::Logger::logError("Bayesian problem requires a gradient of the Mean for each reference evaluation (provided %zu required %zu).", gradientF.size(), refEvaluations.size());
  if (gradientG.size() != refEvaluations.size()) korali::Logger::logError("Bayesian problem requires a gradient of the Standard Deviation for each reference evaluation (provided %zu required %zu).", gradientF.size(), refEvaluations.size());

  std::vector<double> llkgradient(_k->_variables.size(), 0.0);
  for (size_t i = 0; i < _referenceData.size(); ++i)
  {
    if (gradientF[i].size() != _k->_variables.size()) korali::Logger::logError("Bayesian Reference Mean gradient calculation requires gradients of size %zu (provided size %zu)\n", _k->_variables.size(), gradientF[i].size());
    if (gradientG[i].size() != _k->_variables.size()) korali::Logger::logError("Bayesian Reference Standard Deviation gradient calculation requires gradients of size %zu (provided size %zu)\n", _k->_variables.size(), gradientG[i].size());

    double invStdDev = 1.0 / standardDev[i];
    double invStdDev2 = invStdDev * invStdDev;
    double invStdDev3 = invStdDev2 * invStdDev;

    double dif = _referenceData[i] - refEvaluations[i];

    for (size_t d = 0; d < _k->_variables.size(); ++d)
    {
      if (!isfinite(gradientF[i][d])) _k->_logger->logWarning("Normal", "Non-finite value detected in Gradient Mean.\n");
      if (!isfinite(gradientG[i][d])) _k->_logger->logWarning("Normal", "Non-finite value detected in Gradient Standard Deviation.\n");
      double tmpGrad = -invStdDev * gradientG[i][d] + invStdDev2 * dif * gradientF[i][d] + invStdDev3 * dif * dif * gradientG[i][d];
      llkgradient[d] += tmpGrad;
    }
  }

  sample["logLikelihood Gradient"] = llkgradient;
}

void korali::problem::bayesian::Reference::gradientLoglikelihoodPositiveNormal(korali::Sample &sample)
{
  std::vector<double> refEvaluations;
  std::vector<double> standardDev;
  std::vector<std::vector<double>> gradientF;
  std::vector<std::vector<double>> gradientG;

  try
  {
    refEvaluations = sample["Reference Evaluations"].get<std::vector<double>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Reference Evaluations' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its reference evaluations, e.g., sample[\"Reference Evaluations\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  try
  {
    standardDev = sample["Standard Deviation"].get<std::vector<double>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Standard Deviation' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its Standard Deviation, e.g., sample[\"Standard Deviation\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  try
  {
    gradientF = sample["Gradient Mean"].get<std::vector<std::vector<double>>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Gradient Mean' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its Gradient Mean, e.g., sample[\"Gradient Mean\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  try
  {
    gradientG = sample["Gradient Standard Deviation"].get<std::vector<std::vector<double>>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Gradient Standard Deviation' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its Gradient Standard Deviation, e.g., sample[\"Gradient Standard Deviation\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  if (sample["Gradient Mean"].size() != refEvaluations.size()) korali::Logger::logError("Bayesian problem requires a gradient of the Mean for each reference evaluation (provided %zu required %zu).", gradientF.size(), refEvaluations.size());
  if (gradientG.size() != refEvaluations.size()) korali::Logger::logError("Bayesian problem requires a gradient of the Standard Deviation for each reference evaluation (provided %zu required %zu).", gradientF.size(), refEvaluations.size());

  std::vector<double> llkgradient(_k->_variables.size(), 0.0);
  for (size_t i = 0; i < _referenceData.size(); ++i)
  {
    if (gradientF[i].size() != _k->_variables.size()) korali::Logger::logError("Bayesian Reference Mean gradient calculation requires gradients of size %zu (provided size %zu)\n", _k->_variables.size(), gradientF[i].size());
    if (gradientG[i].size() != _k->_variables.size()) korali::Logger::logError("Bayesian Reference Standard Deviation gradient calculation requires gradients of size %zu (provided size %zu)\n", _k->_variables.size(), gradientG[i].size());

    double mu = refEvaluations[i];
    double sig = standardDev[i];

    double invsig = 1.0 / sig;
    double invsig2 = invsig * invsig;
    double invsig3 = invsig2 * invsig;

    double Z = 1.0 - gsl_cdf_gaussian_P(-mu / sig, 1.0);
    double invZ = 1.0 / Z;

    double phims = gsl_ran_gaussian_pdf(-mu / sig, 1.0);

    double dif = _referenceData[i] - refEvaluations[i];

    for (size_t d = 0; d < _k->_variables.size(); ++d)
    {
      if (!isfinite(gradientF[i][d])) _k->_logger->logWarning("Normal", "Non-finite value detected in Gradient Mean.\n");
      if (!isfinite(gradientG[i][d])) _k->_logger->logWarning("Normal", "Non-finite value detected in Gradient Standard Deviation.\n");
      llkgradient[d] += (-invsig * gradientG[i][d] + invsig2 * dif * gradientF[i][d] + invsig3 * dif * dif * gradientG[i][d]);
      llkgradient[d] += invZ * phims * (-1.0 * invsig * gradientF[i][d] + invsig2 * mu * gradientG[i][d]);
    }
  }

  sample["logLikelihood Gradient"] = llkgradient;
}

void korali::problem::bayesian::Reference::gradientLoglikelihoodNegativeBinomial(korali::Sample &sample)
{
  size_t N = _referenceData.size();

  std::vector<double> refEvaluations;
  std::vector<std::vector<double>> gradient;

  try
  {
    refEvaluations = sample["Reference Evaluations"].get<std::vector<double>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Reference Evaluations' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its reference evaluations, e.g., sample[\"Reference Evaluations\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  try
  {
    gradient = sample["Gradient"].get<std::vector<std::vector<double>>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Gradient' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its gradient values, e.g., sample[\"Gradient\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  std::vector<double> llkgradient(N, 0.0);

  double r, m, d, tmpsum;
  for (size_t i = 0; i < N; i++)
  {
    r = sample["Dispersion"][i];
    m = refEvaluations[i];
    d = _referenceData[i];

    tmpsum = r + m;

    for (size_t d = 0; d < _k->_variables.size() - 1; ++d)
    {
      llkgradient[d] += d * (r + gradient[i][d]) / (tmpsum * tmpsum) - r / tmpsum * gradient[i][d];
    }

    llkgradient[_k->_variables.size() - 1] = gsl_sf_psi(r + d) + log(r / tmpsum) + (m - d) / tmpsum - gsl_sf_psi(r);
  }

  sample["logLikelihood Gradient"] = llkgradient;
}

void korali::problem::bayesian::Reference::evaluateFisherInformation(korali::Sample &sample)
{
  double eval = sample["F(x)"];
  if (isfinite(eval))
  {
    if (_likelihoodModel == "Normal")
      fisherInformationLoglikelihoodNormal(sample);
    else if (_likelihoodModel == "Positive Normal")
      fisherInformationLoglikelihoodPositiveNormal(sample);
    else if (_likelihoodModel == "Negative Binomial")
      fisherInformationLoglikelihoodNegativeBinomial(sample);
    else
      korali::Logger::logError("Fisher Information not yet implemented for selected Bayesian problem and log likelihood model.");
  }
  else
  {
    sample["Fisher Information"] = std::vector<double>(_k->_variables.size() * _k->_variables.size(), 0.0);
  }
}

void korali::problem::bayesian::Reference::fisherInformationLoglikelihoodNormal(korali::Sample &sample)
{
  size_t Nd = _referenceData.size();
  size_t Nth = _k->_variables.size();

  std::vector<double> standardDev;
  std::vector<std::vector<double>> gradientF;
  std::vector<std::vector<double>> gradientG;

  try
  {
    standardDev = sample["Standard Deviation"].get<std::vector<double>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Standard Deviation' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its Standard Deviation, e.g., sample[\"Standard Deviation\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  try
  {
    gradientF = sample["Gradient Mean"].get<std::vector<std::vector<double>>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Gradient Mean' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its Gradient Mean, e.g., sample[\"Gradient Mean\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  try
  {
    gradientG = sample["Gradient Standard Deviation"].get<std::vector<std::vector<double>>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Gradient Standard Deviation' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its Gradient Standard Deviation, e.g., sample[\"Gradient Standard Deviation\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  std::vector<double> FIM(Nth * Nth, 0.0);
  for (size_t i = 0; i < Nd; ++i)
  {
    double var = standardDev[i] * standardDev[i];
    double varinv = 1. / var;

    double tmp;
    for (size_t k = 0; k < Nth; ++k)
    {
      for (size_t l = 0; l < k; ++l)
      {
        tmp = varinv * gradientF[i][k] * gradientF[i][l] + 2. * varinv * gradientG[i][k] * gradientG[i][l];
        FIM[k * Nth + l] += tmp;
        FIM[l * Nth + k] += tmp;
      }
      FIM[k * Nth + k] += (varinv * gradientF[i][k] * gradientF[i][k] + 2. * varinv * gradientG[i][k] * gradientG[i][k]);
    }
  }
  sample["Fisher Information"] = FIM;
}

void korali::problem::bayesian::Reference::fisherInformationLoglikelihoodPositiveNormal(korali::Sample &sample)
{
  size_t Nd = _referenceData.size();
  size_t Nth = _k->_variables.size();

  std::vector<double> refEvaluations;
  std::vector<double> standardDev;
  std::vector<std::vector<double>> gradientF;
  std::vector<std::vector<double>> gradientG;

  try
  {
    refEvaluations = sample["Reference Evaluations"].get<std::vector<double>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Reference Evaluations' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its reference evaluations, e.g., sample[\"Reference Evaluations\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  try
  {
    standardDev = sample["Standard Deviation"].get<std::vector<double>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Standard Deviation' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its Standard Deviation, e.g., sample[\"Standard Deviation\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  try
  {
    gradientF = sample["Gradient Mean"].get<std::vector<std::vector<double>>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Gradient Mean' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its Gradient Mean, e.g., sample[\"Gradient Mean\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  try
  {
    gradientG = sample["Gradient Standard Deviation"].get<std::vector<std::vector<double>>>();
  }
  catch (const std::exception &e)
  {
    korali::Logger::logError("Missing or incorrect value of 'Gradient Standard Deviation' returned by the sample evaluation. \n   + Solution: Make sure your model is storing its Gradient Standard Deviation, e.g., sample[\"Gradient Standard Deviation\"] = value(s).\n   + Cause: %s\n", e.what());
  }

  std::vector<double> FIM(Nth * Nth, 0.0);
  for (size_t i = 0; i < Nd; ++i)
  {
    double mu = refEvaluations[i];
    double sig = standardDev[i];
    double var = sig * sig;

    double phims = gsl_ran_ugaussian_pdf(mu / sig);
    double phims2 = phims * phims;

    double Z = 1.0 - gsl_cdf_ugaussian_P(-mu / sig);
    double invZ = 1.0 / Z;
    double invZ2 = invZ * invZ;

    double invvar = 1. / var;
    double invsig3 = invvar / sig;
    double invsig4 = invvar * invvar;
    double invsig5 = invvar * invsig3;

    double Imu = invvar - invZ2 * invvar * phims2 - invZ * mu * invsig3 * phims;
    double Isig = 2. * invvar - 5. * invZ * mu * invsig3 * phims - invZ2 * mu * mu * invsig4 * phims2 - invZ * mu * mu * mu * invsig5 * phims;
    double Ims = invZ * (var + mu * mu) * invsig4 * phims + invZ2 * mu * invsig3 * phims2;

    double tmp;
    for (size_t k = 0; k < Nth; ++k)
    {
      for (size_t l = 0; l < k; ++l)
      {
        tmp = gradientF[i][k] * gradientF[i][l] * Imu + (gradientF[i][k] * 2 * sig * gradientG[i][l] + gradientF[i][l] * 2 * sig * gradientG[i][k]) * Ims + 4 * var * gradientG[i][k] * gradientG[i][l] * Isig;
        FIM[k * Nth + l] += tmp;
        FIM[l * Nth + k] += tmp;
      }
      FIM[k * Nth + k] += (gradientF[i][k] * gradientF[i][k] * Imu + (4 * sig * gradientF[i][k] * gradientG[i][k]) * Ims + 4 * var * gradientG[i][k] * gradientG[i][k] * Isig);
    }
  }
  sample["Fisher Information"] = FIM;
}

void korali::problem::bayesian::Reference::fisherInformationLoglikelihoodNegativeBinomial(korali::Sample &sample)
{
  // TODO
}
