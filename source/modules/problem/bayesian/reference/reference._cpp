#include "modules/conduit/conduit.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/problem/bayesian/reference/reference.hpp"
#include <gsl/gsl_blas.h>
#include <gsl/gsl_cdf.h>
#include <gsl/gsl_matrix.h>
#include <gsl/gsl_sf_gamma.h>

void korali::problem::bayesian::Reference::initialize()
{
  korali::problem::Bayesian::initialize();

  if (_referenceData.size() == 0) _k->_logger->logError("Bayesian (%s) problems require defining reference data.\n", _likelihoodModel.c_str());

  if (_k->_variables.size() < 1) _k->_logger->logError("Bayesian (%s) inference problems require at least one variable.\n", _likelihoodModel.c_str());
}

void korali::problem::bayesian::Reference::evaluateLoglikelihood(korali::Sample &sample)
{
  sample.run(_computationalModel);

  if (sample["Reference Evaluations"].size() != _referenceData.size())
    _k->_logger->logError("This Bayesian (%s) problem requires a %lu-sized result array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), sample["Reference Evaluations"].size());

  if (_likelihoodModel == "Normal")
    loglikelihoodNormal(sample);
  else if (_likelihoodModel == "Negative Binomial")
    loglikelihoodNegativeBinomial(sample);
  else if (_likelihoodModel == "Positive Normal")
    loglikelihoodPositiveNormal(sample);
  else
    _k->_logger->logError("Bayesian problem (%s) not recognized.\n", _likelihoodModel.c_str());
}

double korali::problem::bayesian::Reference::compute_normalized_sse(std::vector<double> f, std::vector<double> g, std::vector<double> y)
{
  double sse = 0.;
  for (size_t i = 0; i < y.size(); i++)
  {
    double diff = (y[i] - f[i]) / g[i];
    sse += diff * diff;
  }
  return sse;
}

void korali::problem::bayesian::Reference::loglikelihoodNormal(korali::Sample &sample)
{
  if (sample["Standard Deviation"].size() != _referenceData.size())
    _k->_logger->logError("This Bayesian (%s) problem requires a %lu-sized Standard Deviation array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), sample["Standard Deviation"].size());

  double sse = compute_normalized_sse(sample["Reference Evaluations"], sample["Standard Deviation"], _referenceData);

  double loglike = 0.;
  double sdev;
  for (size_t i = 0; i < sample["Standard Deviation"].size(); i++)
  {
    sdev = sample["Standard Deviation"][i];
    if (sdev < 0.0) _k->_logger->logError("Negative value (%lf) detected for the Standard Deviation.\n", sdev);
    loglike -= log(sdev);
  }

  loglike -= 0.5 * (_referenceData.size() * _log2pi + sse);
  sample["logLikelihood"] = loglike;
}

void korali::problem::bayesian::Reference::loglikelihoodPositiveNormal(korali::Sample &sample)
{
  double sse = compute_normalized_sse(sample["Reference Evaluations"], sample["Standard Deviation"], _referenceData);

  double loglike = 0.;
  for (size_t i = 0; i < sample["Standard Deviation"].size(); i++)
  {
    double m = sample["Reference Evaluations"][i];
    double s = sample["Standard Deviation"][i];
    loglike -= log(s);
    loglike -= log(1. - gsl_cdf_ugaussian_P(-m / s));
  }
  loglike += -0.5 * (_referenceData.size() * _log2pi + sse);
  sample["logLikelihood"] = loglike;
}

void korali::problem::bayesian::Reference::loglikelihoodNegativeBinomial(korali::Sample &sample)
{
  size_t N = _referenceData.size();
  double loglike = 0.0;

  for (size_t i = 0; i < N; i++)
  {
    loglike -= gsl_sf_lngamma(_referenceData[i] + 1.);
    double m = sample["Reference Evaluations"][i];

    if (m < 0)
    {
      sample["logLikelihood"] = -korali::Inf;
      return;
    }

    double r = sample["Dispersion"][i];
    double p = m / (m + r);
    double y = _referenceData[i];

    loglike += gsl_sf_lngamma(y + r);
    loglike -= gsl_sf_lngamma(r);
    loglike += r * log(1 - p);
    loglike += y * log(p);
  }

  sample["logLikelihood"] = loglike;
}



void korali::problem::bayesian::Reference::evaluateLoglikelihoodGradient(korali::Sample &sample)
{
  double eval = sample["F(x)"];
  if (isfinite(eval))
  {
    if (_likelihoodModel == "Normal")                 gradientLoglikelihoodNormal(sample);
    else if (_likelihoodModel == "Positive Normal")   gradientLoglikelihoodPositiveNormal(sample);
    else if (_likelihoodModel == "Negative Binomial") gradientLoglikelihoodNegativeBinomial(sample);
    else
      _k->_logger->logError("Gradient not yet implemented for selected bayesian problem and log likelihood model.");
  }
  else
  {
    sample["logLikelihood Gradient"] = std::vector<double>(_k->_variables.size(), 0.0);
  }
}

void korali::problem::bayesian::Reference::gradientLoglikelihoodNormal(korali::Sample &sample)
{
  std::vector<double> refEvaluations = sample["Reference Evaluations"];
  std::vector<double> standardDev = sample["Standard Deviation"];
  std::vector<std::vector<double>> gradientF = sample["Gradient Mean"];
  std::vector<std::vector<double>> gradientG = sample["Gradient Standard Deviation"];

  if (sample["Gradient Mean"].size() != refEvaluations.size()) _k->_logger->logError("Bayesian problem requires a gradient of the Mean for each reference evaluation (provided %zu required %zu).", gradientF.size(), refEvaluations.size());
  if (gradientG.size() != refEvaluations.size()) _k->_logger->logError("Bayesian problem requires a gradient of the Standard Deviation for each reference evaluation (provided %zu required %zu).", gradientF.size(), refEvaluations.size());

  std::vector<double> llkgradient(_k->_variables.size(), 0.0);
  for (size_t i = 0; i < _referenceData.size(); ++i)
  {
    if (gradientF[i].size() != _k->_variables.size()) _k->_logger->logError("Bayesian Reference Mean gradient calculation requires gradients of size %zu (provided size %zu)\n", _k->_variables.size(), gradientF[i].size());
    if (gradientG[i].size() != _k->_variables.size()) _k->_logger->logError("Bayesian Reference Standard Deviation gradient calculation requires gradients of size %zu (provided size %zu)\n", _k->_variables.size(), gradientG[i].size());

    double invStdDev = 1.0 / standardDev[i];
    double invStdDev2 = invStdDev * invStdDev;
    double invStdDev3 = invStdDev2 * invStdDev;

    double dif = refEvaluations[i] - _referenceData[i];

    for (size_t d = 0; d < _k->_variables.size(); ++d)
    {
      if (!isfinite(gradientF[i][d])) _k->_logger->logWarning("Normal", "Non-finite value detected in Gradient Mean.\n");
      if (!isfinite(gradientG[i][d])) _k->_logger->logWarning("Normal", "Non-finite value detected in Gradient Standard Deviation.\n");
      double tmpGrad = -invStdDev * gradientG[i][d] - invStdDev2 * dif * gradientF[i][d] + invStdDev3 * dif * dif * gradientG[i][d];
      llkgradient[d] += tmpGrad;
    }
  }

  sample["logLikelihood Gradient"] = llkgradient;
}


void korali::problem::bayesian::Reference::gradientLoglikelihoodPositiveNormal(korali::Sample &sample)
{
  std::vector<double> refEvaluations = sample["Reference Evaluations"];
  std::vector<double> standardDev = sample["Standard Deviation"];
  std::vector<std::vector<double>> gradientF = sample["Gradient Mean"];
  std::vector<std::vector<double>> gradientG = sample["Gradient Standard Deviation"];

  if (sample["Gradient Mean"].size() != refEvaluations.size()) _k->_logger->logError("Bayesian problem requires a gradient of the Mean for each reference evaluation (provided %zu required %zu).", gradientF.size(), refEvaluations.size());
  if (gradientG.size() != refEvaluations.size()) _k->_logger->logError("Bayesian problem requires a gradient of the Standard Deviation for each reference evaluation (provided %zu required %zu).", gradientF.size(), refEvaluations.size());

  std::vector<double> llkgradient(_k->_variables.size(), 0.0);
  for (size_t i = 0; i < _referenceData.size(); ++i)
  {
    if (gradientF[i].size() != _k->_variables.size()) _k->_logger->logError("Bayesian Reference Mean gradient calculation requires gradients of size %zu (provided size %zu)\n", _k->_variables.size(), gradientF[i].size());
    if (gradientG[i].size() != _k->_variables.size()) _k->_logger->logError("Bayesian Reference Standard Deviation gradient calculation requires gradients of size %zu (provided size %zu)\n", _k->_variables.size(), gradientG[i].size());

    double mu  = refEvaluations[i];
    double sig = standardDev[i];
    
    double invsig  = 1.0 / sig;
    double invsig2 = invsig * invsig;
    double invsig3 = invsig2 * invsig;
 
    double Z     = 1.0 - gsl_cdf_gaussian_P(-mu, invsig);
    double invZ  = 1.0/Z;
    double phims = gsl_ran_gaussian_pdf(-mu, sig);

    double dif = _referenceData[i] - refEvaluations[i];

    for (size_t d = 0; d < _k->_variables.size(); ++d)
    {
      if (!isfinite(gradientF[i][d])) _k->_logger->logWarning("Normal", "Non-finite value detected in Gradient Mean.\n");
      if (!isfinite(gradientG[i][d])) _k->_logger->logWarning("Normal", "Non-finite value detected in Gradient Standard Deviation.\n");
      llkgradient[d] += (-invsig * gradientG[i][d] + invZ * phims * ( -1.0 * invsig * gradientF[i][d] + invsig2 * mu * gradientG[i][d] ) + dif * ( invsig2 * gradientF[i][d] + invsig3 * dif * gradientG[i][d] ));
    }
  }

  sample["logLikelihood Gradient"] = llkgradient;
}


void korali::problem::bayesian::Reference::gradientLoglikelihoodNegativeBinomial(korali::Sample &sample)
{
  size_t N = _referenceData.size();

  std::vector<std::vector<double>> gradient = sample["Gradient"];
  std::vector<double> refEvaluations = sample["Reference Evaluations"];

  std::vector<double> llkgradient(N, 0.0);

  double r, m, d, tmpsum;
  for (size_t i = 0; i < N; i++)
  {
    r = sample["Dispersion"][i];
    m = refEvaluations[i];
    d = _referenceData[i];

    tmpsum = r + m;

    for (size_t d = 0; d < _k->_variables.size() - 1; ++d)
    {
      llkgradient[d] += d * (r + gradient[i][d]) / (tmpsum * tmpsum) - r / tmpsum * gradient[i][d];
    }

    llkgradient[_k->_variables.size() - 1] = gsl_sf_psi(r + d) + log(r / tmpsum) + (m - d) / tmpsum - gsl_sf_psi(r);
  }

  sample["logLikelihood Gradient"] = llkgradient;
}


void korali::problem::bayesian::Reference::evaluateFisherInformation(korali::Sample &sample)
{
  double eval = sample["F(x)"];
  if (isfinite(eval))
  {
    if (_likelihoodModel == "Normal")                 fisherInformationLoglikelihoodNormal(sample);
    else if (_likelihoodModel == "Positive Normal")   fisherInformationLoglikelihoodPositiveNormal(sample);
    else if (_likelihoodModel == "Negative Binomial") fisherInformationLoglikelihoodNegativeBinomial(sample);
    else
      _k->_logger->logError("Fisher Information not yet implemented for selected Bayesian problem and log likelihood model.");
  }
  else
  {
    sample["Fisher Information"] = std::vector<double>(_k->_variables.size() * _k->_variables.size(), 0.0);
  }
}

void korali::problem::bayesian::Reference::fisherInformationLoglikelihoodNormal(korali::Sample &sample)
{
  size_t Nd = _referenceData.size();
  size_t Nth = _k->_variables.size();

  std::vector<double> standardDev = sample["Standard Deviation"];
  std::vector<std::vector<double>> gradientF = sample["Gradient Mean"];
  std::vector<std::vector<double>> gradientG = sample["Gradient Standard Deviation"];

  std::vector<double> FIM(Nth * Nth, 0.0);
  for (size_t i = 0; i < Nd; ++i)
  {
    double var = standardDev[i] * standardDev[i];
    double varinv = 1. / var;
    
    double tmp;
    for (size_t k = 0; k < Nth; ++k)
    {
      for (size_t l = 0; l < k; ++l)
      {
        tmp = varinv * gradientF[i][k] * gradientF[i][l] + 2. * varinv * gradientG[i][k] * gradientG[i][l];
        FIM[k * Nth + l] += tmp;
        FIM[l * Nth + k] += tmp;
      }
      FIM[k * Nth + k] = var * gradientF[i][k] * gradientF[i][k] + 2. * varinv * gradientG[i][k] * gradientG[i][k];
    }
  }
  sample["Fisher Information"] = FIM;
}


void korali::problem::bayesian::Reference::fisherInformationLoglikelihoodPositiveNormal(korali::Sample &sample)
{
  size_t Nd = _referenceData.size();
  size_t Nth = _k->_variables.size();

  std::vector<double> refEvaluations = sample["Reference Evaluations"];
  std::vector<double> standardDev = sample["Standard Deviation"];
  std::vector<std::vector<double>> gradientF = sample["Gradient Mean"];
  std::vector<std::vector<double>> gradientG = sample["Gradient Standard Deviation"];

  std::vector<double> FIM(Nth * Nth, 0.0);
  for (size_t i = 0; i < Nd; ++i)
  { 
    double mu    = refEvaluations[i];
    double sig   = standardDev[i];
    double var   = sig*sig;

    double phims  = gsl_ran_gaussian_pdf(mu, sig);
    double phims2 = phims*phims;
 
    double Z     = 1.0 - gsl_cdf_gaussian_P(-mu, sig);
    double invZ  = 1.0/Z;
    double invZ2 = invZ*invZ;

    double invvar  = 1./var;
    double invsig3 = 1./(var*sig);
    double invsig4 = 1./(var*var);
    double invsig5 = invvar * invsig3;

    double Imu  = invvar - invZ2*invvar*phims2 - invZ*mu*invsig3*phims;
    double Isig = 2.*invvar - 5.*invZ*mu*invsig3*phims - invZ2*mu*mu*invsig4*phims2 - invZ*mu*mu*mu*invsig5*phims;
    double Ims  = invZ*(var+mu*mu)*invsig4*phims + invZ2*mu*invsig3*phims2;

    double tmp;
    for (size_t k = 0; k < Nth; ++k)
    {
      for (size_t l = 0; l < k; ++l)
      {
        tmp = gradientF[i][k]*gradientF[i][l]*Imu+(gradientF[i][k]*gradientG[i][l]+gradientF[i][l]*gradientG[i][k])*Ims+gradientG[i][k]*gradientG[i][l]*Isig;
        FIM[k * Nth + l] += tmp;
        FIM[l * Nth + k] += tmp;
      }
      FIM[k * Nth + k] = gradientF[i][k]*gradientF[i][k]*Imu+(gradientF[i][k]*gradientG[i][k]+gradientF[i][k]*gradientG[i][k])*Ims+gradientG[i][k]*gradientG[i][k]*Isig;
    }
  }
  sample["Fisher Information"] = FIM;
}


void korali::problem::bayesian::Reference::fisherInformationLoglikelihoodNegativeBinomial(korali::Sample &sample)
{
    // TODO
}
